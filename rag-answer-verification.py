import streamlit as st
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
import os
from dotenv import load_dotenv
import tempfile

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

if not openai_api_key:
    st.error("OpenAI API í‚¤ë¥¼ .env íŒŒì¼ì— ì„¤ì •í•´ ì£¼ì„¸ìš”.")
    st.stop()

@st.cache_resource
def load_and_process_docs(uploaded_file):
    """
    ì—…ë¡œë“œëœ PDF íŒŒì¼ì„ ìž„ì‹œë¡œ ì €ìž¥í•˜ê³ , í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•œ ë’¤, ë²¡í„° DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if uploaded_file:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name
        
        loader = PyPDFLoader(tmp_path)
        documents = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        docs = text_splitter.split_documents(documents)
        
        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        vector_store = FAISS.from_documents(docs, embeddings)
        
        # ìž„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(tmp_path)
        
        return vector_store, docs
    return None, None

# Streamlit ì•± UI ì„¤ì •
st.title("ðŸ“š RAG + ë‹µë³€ í‰ê°€ ì• í”Œë¦¬ì¼€ì´ì…˜")
st.write("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³ , ë‹µë³€ì˜ ì‹ ë¢°ë„ë¥¼ 'ìƒ, ì¤‘, í•˜' ë“±ê¸‰ìœ¼ë¡œ í™•ì¸í•˜ì„¸ìš”.")

# PDF íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf")

if uploaded_file:
    # ë¬¸ì„œ ë¡œë”© ë° ë²¡í„° ì €ìž¥ì†Œ ìƒì„± (ìºì‹±)
    vector_store, all_docs = load_and_process_docs(uploaded_file)

    if vector_store:
        # 1. RAG ì²´ì¸ êµ¬ì„±
        retriever = vector_store.as_retriever()
        rag_chain = RetrievalQA.from_chain_type(
            llm=ChatOpenAI(model_name="gpt-4o", temperature=0, openai_api_key=openai_api_key),
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True
        )
        
        # 2. ë‹µë³€ í‰ê°€ í”„ë¡¬í”„íŠ¸ ë° LLM ì •ì˜
        verification_template = """
        ì œê³µëœ 'ë¬¸ì„œ ë‚´ìš©'ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ëž˜ 'ë‹µë³€'ì˜ ì‚¬ì‹¤ì„±ê³¼ ì •í™•ì„±ì„ í‰ê°€í•˜ê³  ì ìˆ˜ë¥¼ 1ì ì—ì„œ 5ì  ì‚¬ì´ë¡œ ë§¤ê²¨ì£¼ì„¸ìš”.
        1: ë¬¸ì„œì™€ ì „í˜€ ê´€ë ¨ ì—†ìŒ.
        2: ë¬¸ì„œì— ì¼ë¶€ ì •ë³´ê°€ ìžˆì§€ë§Œ ë‹µë³€ì— ê·¼ê±°ê°€ ë¶€ì¡±í•¨.
        3: ë‹µë³€ì˜ ì ˆë°˜ ì •ë„ê°€ ë¬¸ì„œì— ê·¼ê±°í•˜ë©°, ì¼ë¶€ëŠ” ë¶ˆë¶„ëª…í•¨.
        4: ë‹µë³€ì˜ ëŒ€ë¶€ë¶„ì´ ë¬¸ì„œì— ì •í™•ížˆ ê·¼ê±°í•¨.
        5: ë‹µë³€ì˜ ëª¨ë“  ë‚´ìš©ì´ ë¬¸ì„œì— ì™„ë²½í•˜ê³  ì •í™•í•˜ê²Œ ê·¼ê±°í•¨.
        
        í‰ê°€ ì ìˆ˜ë§Œ ìˆ«ìžë¡œ ë‹µí•˜ì„¸ìš”. (ì˜ˆ: 5)
        
        ---
        ë¬¸ì„œ ë‚´ìš©:
        {context}
        
        ---
        ë‹µë³€:
        {answer}
        """
        verification_prompt = PromptTemplate.from_template(verification_template)
        verification_llm = ChatOpenAI(model_name="gpt-4o", temperature=0, openai_api_key=openai_api_key)

        # 3. ì‚¬ìš©ìž ì§ˆë¬¸ ìž…ë ¥ ë° ì²˜ë¦¬
        question = st.text_input("ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”:", key="user_question")
        
        if question:
            with st.spinner("ðŸ” ë‹µë³€ ë° í‰ê°€ ìƒì„± ì¤‘..."):
                # RAGë¥¼ í†µí•œ ë‹µë³€ ìƒì„±
                rag_result = rag_chain.invoke({"query": question})
                answer = rag_result['result']
                source_documents = rag_result['source_documents']
                
                # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìžì—´ë¡œ ê²°í•©
                source_context = "\n\n".join([doc.page_content for doc in source_documents])

                # ë‹µë³€ í‰ê°€
                verification_result = verification_llm.invoke(verification_prompt.format(
                    context=source_context,
                    answer=answer
                ))
                
                # LLMì˜ ë‹µë³€ì—ì„œ ì ìˆ˜ë§Œ ì¶”ì¶œ (ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨)
                try:
                    score = int(verification_result.content.strip())
                    if score >= 4:
                        grade = "ìƒ"
                        st.success(f"**ë‹µë³€:** {answer}")
                        st.info(f"âœ… **í‰ê°€ ê²°ê³¼:** {grade} (ì •í™•í•˜ê³  ë¬¸ì„œì— ê·¼ê±°í•¨)")
                    elif score == 3:
                        grade = "ì¤‘"
                        st.warning(f"**ë‹µë³€:** {answer}")
                        st.info(f"ðŸŸ¡ **í‰ê°€ ê²°ê³¼:** {grade} (ì¼ë¶€ ê·¼ê±°ê°€ ë¶€ì¡±í•  ìˆ˜ ìžˆìŒ)")
                    else:
                        grade = "í•˜"
                        st.error(f"**ë‹µë³€:** {answer}")
                        st.info(f"âŒ **í‰ê°€ ê²°ê³¼:** {grade} (ë¬¸ì„œì™€ ê´€ë ¨ì„±ì´ ë‚®ê±°ë‚˜ í™˜ê° ê°€ëŠ¥ì„±)")
                except (ValueError, IndexError):
                    st.warning(f"**ë‹µë³€:** {answer}")
                    st.error("í‰ê°€ ê²°ê³¼ë¥¼ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                st.markdown("---")
                st.markdown("### ðŸ“– ì°¸ê³  ë¬¸ì„œ")
                for i, doc in enumerate(source_documents):
                    st.markdown(f"**ë¬¸ì„œ {i+1}:** (íŽ˜ì´ì§€ {doc.metadata.get('page', 'N/A')})")
                    st.markdown(f"> {doc.page_content}")