import streamlit as st
import os
import json
import re 
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from typing import List

from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# -------------------- í™˜ê²½ ë³€ìˆ˜ --------------------
script_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(script_dir)
dotenv_path = os.path.join(parent_dir, ".env")
load_dotenv(dotenv_path=dotenv_path)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if OPENAI_API_KEY:
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
else:
    st.error(f"ğŸš¨ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”. (ê²½ë¡œ: {dotenv_path})")
    st.stop()

RAG_MODEL_NAME = "gpt-3.5-turbo"
EVAL_MODEL_NAME = "gpt-4o-mini"

# -------------------- LLM ì´ˆê¸°í™” --------------------
@st.cache_resource(show_spinner=False)
def initialize_llms(api_key):
    rag_llm = ChatOpenAI(model=RAG_MODEL_NAME, temperature=0.1, api_key=api_key)
    eval_llm = ChatOpenAI(model=EVAL_MODEL_NAME, temperature=0.0, api_key=api_key, request_timeout=60)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large", api_key=api_key)
    return rag_llm, eval_llm, embeddings

rag_llm, eval_llm, embeddings = initialize_llms(OPENAI_API_KEY)

# -------------------- JSON ì•ˆì „ íŒŒì‹± --------------------
def safe_json_parse(raw_content: str):
    try:
        return json.loads(raw_content) 
    except json.JSONDecodeError:
        match = re.search(r'```json\s*(\{.*?\})\s*```', raw_content, re.DOTALL) or re.search(r'(\{.*\})', raw_content, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError as e:
                raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)[:50]}...")
        else:
            raise ValueError(f"JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {raw_content[:100]}...")

# -------------------- ë¬¸ì„œ ì²˜ë¦¬ --------------------
def load_and_process_docs(uploaded_file):
    file_extension = os.path.splitext(uploaded_file.name)[1].lower()
    temp_file_path = f"./temp_doc_{os.getpid()}_{os.path.basename(uploaded_file.name)}"
    
    with open(temp_file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    if file_extension == ".pdf":
        loader = PyPDFLoader(temp_file_path)
    elif file_extension in [".txt", ".md"]:
        loader = TextLoader(temp_file_path, encoding='utf-8')
    else:
        os.remove(temp_file_path)
        return None, "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. (PDF, TXT, MDë§Œ ì§€ì›)"

    try:
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = text_splitter.split_documents(documents)
    except Exception as e:
        os.remove(temp_file_path)
        return None, f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

    os.remove(temp_file_path)
    vectorstore = FAISS.from_documents(chunks, embeddings)
    
    return vectorstore, f"ë¬¸ì„œ ë¡œë“œ ë° ì¸ë±ì‹± ì™„ë£Œ! ì´ {len(chunks)}ê°œ ì²­í¬."

# -------------------- RAG ë‹µë³€ ìƒì„± --------------------
def retrieve_and_generate(query, vectorstore):
    retriever = vectorstore.as_retriever(search_kwargs={"k": 8})
    retrieved_docs = retriever.invoke(query)
    
    context = "\n---\n".join([doc.page_content for doc in retrieved_docs])
    source_info = [{"content": doc.page_content, "source": doc.metadata.get('source', 'ì—…ë¡œë“œ ë¬¸ì„œ'), "page": doc.metadata.get('page', 'N/A')} for doc in retrieved_docs]

    rag_prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ ì œê³µëœ 'ë§¥ë½'ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹µë³€ì€ ë§¥ë½ì˜ ë‚´ìš©ì„ ì •í™•íˆ ì¸ìš©í•´ì•¼ í•©ë‹ˆë‹¤. "
                   "ë§Œì•½ ë§¥ë½ì— ë‹µë³€í•  ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ë‹¤ë©´ 'ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”."), 
        ("user", "ë§¥ë½:\n{context}\n\nì§ˆë¬¸: {question}")
    ])
    
    chain = rag_prompt | rag_llm | StrOutputParser()
    response = chain.invoke({"context": context, "question": query})
    
    return response, context, source_info, retrieved_docs

# -------------------- Precision í‰ê°€ --------------------
class PrecisionEval(BaseModel):
    relevant: bool
    rationale: str

def evaluate_precision(query: str, retrieved_docs: list):
    evaluation_results = []
    total_chunks = len(retrieved_docs)
    relevant_chunks = 0

    prompt_template = PromptTemplate(
        input_variables=["query", "chunk"],
        template=(
            "ì§ˆë¬¸: {query}\n"
            "ì²­í¬ ë‚´ìš©: {chunk}\n"
            "ìœ„ ì²­í¬ê°€ ì§ˆë¬¸ì— ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.\n"
            "ì˜ˆì‹œ: {{\"relevant\": true, \"rationale\": \"ê´€ë ¨ ê·¼ê±° ë‚´ìš©\"}}"
        )
    )

    for i, doc in enumerate(retrieved_docs):
        try:
            input_text = prompt_template.format(query=query, chunk=doc.page_content)
            structured_llm = eval_llm.with_structured_output(PrecisionEval)
            eval_result: PrecisionEval = structured_llm.invoke(input_text)

            relevant = getattr(eval_result, "relevant", False)
            rationale = getattr(eval_result, "rationale", "ì´ìœ  ì—†ìŒ")
            if relevant:
                relevant_chunks += 1

            evaluation_results.append({
                "chunk_index": i + 1,
                "content": doc.page_content,
                "relevant": relevant,
                "score": 1.0 if relevant else 0.0,
                "rationale": rationale
            })
        except Exception as e:
            evaluation_results.append({
                "chunk_index": i + 1,
                "content": doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content,
                "relevant": False,
                "score": 0.0,
                "rationale": f"LLM í‰ê°€ ì˜¤ë¥˜: {e}"
            })

    precision_score = relevant_chunks / total_chunks if total_chunks > 0 else 0.0
    return {
        "score": precision_score,
        "total_chunks": total_chunks,
        "relevant_chunks": relevant_chunks,
        "detailed_results": evaluation_results
    }

# -------------------- Faithfulness í‰ê°€ --------------------
class FactItem(BaseModel):
    fact: str
    supported: bool
    fact_rationale: str

class FaithfulnessEval(BaseModel):
    overall_rationale: str
    facts: List[FactItem]

def evaluate_faithfulness(answer: str, context: str):
    try:
        structured_llm = eval_llm.with_structured_output(FaithfulnessEval)
        prompt = f"ë§¥ë½: {context}\në‹µë³€: {answer}\nìœ„ ë‹µë³€ì˜ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ í‰ê°€í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”."
        eval_result: FaithfulnessEval = structured_llm.invoke(prompt)
        facts = eval_result.facts
        total_facts = len(facts)
        supported_facts = sum(1 for f in facts if f.supported)
        score = supported_facts / total_facts if total_facts > 0 else 0.0
        return {
            "score": score,
            "overall_rationale": eval_result.overall_rationale,
            "verified_facts": [f.dict() for f in facts],
        }
    except Exception as e:
        return {"score": 0.0, "overall_rationale": f"í‰ê°€ ì˜¤ë¥˜: {e}", "verified_facts": []}

# -------------------- Streamlit UI --------------------
st.set_page_config(page_title="RAGAS ì‹¬ì¸µ ë¶„ì„ ì•±", layout="wide")
st.title("âœ¨ RAGAS ê²€ì¦ ì‹¬ì¸µ ë¶„ì„")
st.markdown(f"### `{RAG_MODEL_NAME}`ë¡œ ë‹µë³€ ìƒì„±, `{EVAL_MODEL_NAME}`ë¡œ í‰ê°€ ì§„í–‰")
st.markdown("---")

# íŒŒì¼ ì—…ë¡œë“œ
with st.sidebar:
    st.header("1. ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("PDF, TXT, MD íŒŒì¼ ì—…ë¡œë“œ", type=["pdf","txt","md"])
    if uploaded_file:
        if "vectorstore" not in st.session_state or st.session_state.get("uploaded_file_name") != uploaded_file.name:
            with st.spinner("ë¬¸ì„œ ì²˜ë¦¬ ë° ì¸ë±ì‹± ì¤‘..."):
                vectorstore, status_message = load_and_process_docs(uploaded_file)
                if vectorstore:
                    st.session_state.vectorstore = vectorstore
                    st.session_state.status = status_message
                    st.session_state.uploaded_file_name = uploaded_file.name
                else:
                    st.error(status_message)
                    st.session_state.status = status_message
    if "status" in st.session_state:
        st.info(st.session_state.status)

# ì§ˆë¬¸ ì…ë ¥ ë° ì‹¤í–‰
if "vectorstore" in st.session_state:
    st.header("2. ì§ˆë¬¸í•˜ê¸°")
    question = st.text_input("ì§ˆë¬¸ ì…ë ¥:", placeholder="ì„¸íƒê¸° ìˆ˜í‰ ë§ì¶”ëŠ” ë°©ë²•")
    if st.button("ë‹µë³€ ìƒì„± ë° í‰ê°€ ì‹¤í–‰"):
        if question:
            st.session_state.answer = None
            st.session_state.precision_evaluation = None
            st.session_state.faithfulness_evaluation = None
            with st.spinner("RAGAS ì‹¬ì¸µ ê²€ì¦ ì¤‘..."):
                answer, context, source_info, retrieved_docs = retrieve_and_generate(question, st.session_state.vectorstore)
                precision_result = evaluate_precision(question, retrieved_docs)
                faithfulness_result = evaluate_faithfulness(answer, context)

                st.session_state.answer = answer
                st.session_state.context = context
                st.session_state.source_info = source_info
                st.session_state.precision_evaluation = precision_result
                st.session_state.faithfulness_evaluation = faithfulness_result
                st.success("âœ… ì‹¬ì¸µ ê²€ì¦ ì™„ë£Œ!")

# ê²°ê³¼ í‘œì‹œ
if st.session_state.get("answer"):
    st.markdown("---")
    precision_eval = st.session_state.precision_evaluation
    st.subheader("ğŸ’¡ ë¦¬íŠ¸ë¦¬ë²Œ í‰ê°€ì§€í‘œ (Context Precision)")
    col_score, col_summary = st.columns([1,2])
    with col_score:
        st.metric("ë§¥ë½ ì •í™•ë„ ì ìˆ˜", f"{precision_eval['score']:.2f}")
    with col_summary:
        st.markdown(f"ì´ {precision_eval['total_chunks']}ê°œ ì²­í¬ ì¤‘ {precision_eval['relevant_chunks']}ê°œ ê´€ë ¨ ìˆìŒ")

    with st.expander("ì²­í¬ë³„ ìƒì„¸ í‰ê°€"):
        precision_data = []
        for item in precision_eval['detailed_results']:
            precision_data.append({
                "ì²­í¬ ë²ˆí˜¸": item['chunk_index'],
                "ê´€ë ¨ì„±": "âœ… ê´€ë ¨ ìˆìŒ" if item['relevant'] else "âŒ ê´€ë ¨ ì—†ìŒ",
                "ì‹¬ì‚¬ê´€ ê·¼ê±°": item['rationale'],
                "ì²­í¬ ë‚´ìš© (ì¼ë¶€)": item['content'][:150]+"..." if len(item['content'])>150 else item['content']
            })
        st.dataframe(precision_data, hide_index=True, use_container_width=True)

    st.markdown("---")
    faithfulness_eval = st.session_state.faithfulness_evaluation
    answer = st.session_state.answer
    total_facts = len(faithfulness_eval['verified_facts'])
    supported_facts = sum(1 for f in faithfulness_eval['verified_facts'] if f.get('supported', False))
    st.subheader("ğŸ’¡ ìƒì„±ëœ ë‹µë³€ (Faithfulness)")
    st.success(answer)
    col_score, col_rationale = st.columns([1,2])
    with col_score:
        st.metric(f"({supported_facts}/{total_facts}) Supported", f"{faithfulness_eval['score']:.2f}")
    with col_rationale:
        st.info(faithfulness_eval['overall_rationale'])